#!/usr/bin/env python3
import os
import pickle
import json
import re
import numpy as np
from tqdm import tqdm
import argparse
from transformers import AutoTokenizer
from vllm import SamplingParams, LLM
import time
import glob

def is_huggingface_model_id(model_path: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºHugging Faceæ¨¡å‹ID
    å¦‚æœè·¯å¾„å­˜åœ¨äºæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œåˆ™è®¤ä¸ºæ˜¯æœ¬åœ°è·¯å¾„ï¼›å¦åˆ™è®¤ä¸ºæ˜¯HFæ¨¡å‹ID
    """
    return not os.path.exists(model_path)

def get_model_name_from_path(model_path: str) -> str:
    """
    ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°ä½œä¸ºç›®å½•å
    """
    if is_huggingface_model_id(model_path):
        # HFæ¨¡å‹IDï¼Œå¦‚ "microsoft/DialoGPT-medium" -> "microsoft_DialoGPT-medium"
        return model_path.replace('/', '_').replace('\\', '_')
    else:
        # æœ¬åœ°è·¯å¾„ï¼Œå–æœ€åä¸€ä¸ªç›®å½•åï¼Œå¦‚ "/path/to/final_model" -> "final_model"
        return os.path.basename(os.path.normpath(model_path))

def truncate_context(context: str, tokenizer, max_tokens: int = 600) -> str:
    """
    æ›´é«˜æ•ˆåœ°æˆªæ–­contextï¼Œåªç¼–ç å¿…è¦éƒ¨åˆ†ï¼Œé¿å…é‡å¤å’Œå…¨é‡encodeã€‚
    ä¿ç•™å‰300å’Œå300 tokensï¼Œä¸­é—´ä¸é‡å ã€‚
    """
    max_each = max_tokens // 2  # é»˜è®¤å„ä¿ç•™300ä¸ª
    rough_char_per_token = 4  # ä¼°è®¡æ¯ä¸ªtokençº¦4å­—ç¬¦ï¼Œå¯æ ¹æ®è¯­è¨€ç±»å‹å¾®è°ƒ

    head_char_len = max_each * rough_char_per_token
    tail_char_len = max_each * rough_char_per_token

    total_len = len(context)

    # å¦‚æœå­—ç¬¦é•¿åº¦å¤ªçŸ­ï¼Œè¯´æ˜æœ¬æ¥å°±ä¸éœ€è¦æˆªæ–­
    if total_len <= head_char_len + tail_char_len:
        return context

    # åˆ†åˆ«æˆªå–å‰åæ®µï¼Œç¡®ä¿ä¸­é—´ä¸é‡å 
    front_chunk = context[:head_char_len]
    back_chunk = context[-tail_char_len:] if total_len > head_char_len + tail_char_len else context[head_char_len:]

    # åˆ†åˆ«tokenize
    front_tokens = tokenizer.encode(front_chunk, add_special_tokens=False)[:max_each]
    back_tokens = tokenizer.encode(back_chunk, add_special_tokens=False)[-max_each:]

    # decode
    front_text = tokenizer.decode(front_tokens, skip_special_tokens=True)
    back_text = tokenizer.decode(back_tokens, skip_special_tokens=True)

    # ç»„åˆï¼ˆä¸­é—´ç”¨çœç•¥æ ‡è®°ï¼‰
    return front_text + "\n\n[...]\n\n" + back_text

def load_instagger_vllm(model_path, tensor_parallel_size=1):
    """ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹å’Œ tokenizer - æœ¬åœ°æ¨¡å‹å’ŒHFæ¨¡å‹"""
    
    is_hf_model = is_huggingface_model_id(model_path)
    
    if is_hf_model:
        print(f"Loading Hugging Face model: {model_path}")
    else:
        print(f"Loading local model from: {model_path}")
        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Local model path not found: {model_path}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
        required_files = ['config.json']
        for file in required_files:
            file_path = os.path.join(model_path, file)
            if not os.path.exists(file_path):
                print(f"Warning: {file} not found in {model_path}")
    
    # é…ç½®vLLMå‚æ•°
    llm_kwargs = {
        "model": model_path,
        "tensor_parallel_size": tensor_parallel_size,
        "gpu_memory_utilization": 0.8,
        "disable_log_stats": True,
        "trust_remote_code": True,
        "enforce_eager": True,
        "max_model_len": 2048,
    }
    
    # åªå¯¹æœ¬åœ°æ¨¡å‹è®¾ç½®download_dir=None
    if not is_hf_model:
        llm_kwargs["download_dir"] = None
    
    # å¤šGPUé…ç½®
    if tensor_parallel_size > 1:
        llm_kwargs["distributed_executor_backend"] = "mp"
        print(f"Using multi-GPU mode with {tensor_parallel_size} GPUs")
    else:
        print("Using single GPU mode")
    
    try:
        print("Initializing vLLM engine...")
        llm = LLM(**llm_kwargs)
        print("vLLM engine loaded successfully")
    except Exception as e:
        print(f"Failed to load with tensor_parallel_size={tensor_parallel_size}: {e}")
        print("Trying single GPU mode...")
        # å›é€€åˆ°å•GPUæ¨¡å¼
        llm_kwargs["tensor_parallel_size"] = 1
        if "distributed_executor_backend" in llm_kwargs:
            del llm_kwargs["distributed_executor_backend"]
        try:
            llm = LLM(**llm_kwargs)
            print("Successfully loaded in single GPU mode")
        except Exception as e2:
            print(f"Failed to load even in single GPU mode: {e2}")
            raise e2

    print("Loading tokenizer...")
    try:
        tokenizer_kwargs = {
            "use_fast": False,
            "trust_remote_code": True,
        }
        
        # åªå¯¹æœ¬åœ°æ¨¡å‹å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        if not is_hf_model:
            tokenizer_kwargs["local_files_only"] = True
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, **tokenizer_kwargs)
        print("Tokenizer loaded successfully")
        
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        if not is_hf_model:
            print("Trying without local_files_only...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                use_fast=False,
                trust_remote_code=True,
            )
        else:
            raise e
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("Set pad_token to eos_token")

    # æ‰“å°EOS tokenä¿¡æ¯
    print(f"EOS token: {tokenizer.eos_token}")
    print(f"EOS token ID: {tokenizer.eos_token_id}")

    return llm, tokenizer

def extract_tags_with_explanations(tags_text):
    """æå–æ ‡ç­¾å’Œè§£é‡Š - æ”¹è¿›çš„JSONè§£æ"""
    try:
        # å¯»æ‰¾JSONæ•°ç»„
        json_pattern = r'\[.*?\]'
        json_matches = re.findall(json_pattern, tags_text, re.DOTALL)
        
        if json_matches:
            json_str = json_matches[-1]
            try:
                parsed_json = json.loads(json_str)
                if isinstance(parsed_json, list):
                    valid_tags = []
                    for item in parsed_json:
                        if isinstance(item, dict) and "tag" in item and "explanation" in item:
                            valid_tags.append({
                                "tag": str(item["tag"]).strip(),
                                "explanation": str(item["explanation"]).strip()
                            })
                    return valid_tags[:5]
            except json.JSONDecodeError:
                pass
        
        # å¯»æ‰¾å•ä¸ªJSONå¯¹è±¡
        single_json_pattern = r'\{[^{}]*"tag"[^{}]*"explanation"[^{}]*\}'
        single_matches = re.findall(single_json_pattern, tags_text)
        
        if single_matches:
            valid_tags = []
            for match in single_matches:
                try:
                    item = json.loads(match)
                    if "tag" in item and "explanation" in item:
                        valid_tags.append({
                            "tag": str(item["tag"]).strip(),
                            "explanation": str(item["explanation"]).strip()
                        })
                except:
                    continue
            return valid_tags[:5]
        
        # å›é€€æ–¹æ¡ˆï¼šç®€å•è§£æ
        return _fallback_parse(tags_text)
        
    except Exception as e:
        print(f"Error extracting tags with explanations: {e}")
        return []

def _fallback_parse(response: str):
    """å›é€€è§£ææ–¹æ³•"""
    try:
        tags = []
        lines = response.split('\n')
        current_tag = None
        current_explanation = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if 'tag' in line.lower() and ':' in line:
                if current_tag and current_explanation:
                    tags.append({"tag": current_tag, "explanation": current_explanation})
                
                parts = line.split(':', 1)
                if len(parts) == 2:
                    current_tag = parts[1].strip().replace('"', '').replace("'", "")
                    current_explanation = None
            
            elif 'explanation' in line.lower() and ':' in line:
                parts = line.split(':', 1)
                if len(parts) == 2:
                    current_explanation = parts[1].strip().replace('"', '').replace("'", "")
            
            elif current_tag and not current_explanation and line:
                current_explanation = line.replace('"', '').replace("'", "")
        
        if current_tag and current_explanation:
            tags.append({"tag": current_tag, "explanation": current_explanation})
        
        return tags[:5] if tags else [{"tag": "General", "explanation": "Unable to parse specific tags"}]
    except:
        return [{"tag": "Error", "explanation": "Failed to parse response"}]

def extract_tags(tags_text):
    tags_with_explanations = extract_tags_with_explanations(tags_text)
    return [item["tag"] for item in tags_with_explanations]

def append_results_to_file(output_file, new_results):
    """è¿½åŠ ç»“æœåˆ°æ–‡ä»¶"""
    if not new_results:
        return 0
    
    existing_results = []
    if os.path.exists(output_file):
        try:
            with open(output_file, 'rb') as f:
                existing_results = pickle.load(f)
        except Exception as e:
            print(f"Warning: Could not load existing results, starting fresh: {e}")
            existing_results = []
    
    all_results = existing_results + new_results
    
    try:
        with open(output_file, 'wb') as f:
            pickle.dump(all_results, f)
        print(f"Saved {len(new_results)} new results. Total: {len(all_results)} samples")
        return len(all_results)
    except Exception as e:
        print(f"Error saving results: {e}")
        return len(existing_results)

def run_inference_vllm(model_path: str, data: list, output_file: str,
                       save_raw_text=True, save_interval=64, batch_size=32,
                       tensor_parallel_size=1):
    
    print("Loading model...")
    llm, tokenizer = load_instagger_vllm(model_path, tensor_parallel_size)
    print("Model loaded successfully")

    print(f"Running vLLM inference on {len(data)} samples (batch_size={batch_size})...")
    
    initial_count = 0
    if os.path.exists(output_file):
        try:
            with open(output_file, 'rb') as f:
                existing_results = pickle.load(f)
                initial_count = len(existing_results)
            print(f"Found existing results file with {initial_count} samples")
        except:
            print("Starting with empty results file")
    
    start_time = time.time()
    batch_results = [] 
    total_processed = 0
    
    for start in tqdm(range(0, len(data), batch_size), desc="Processing batches"):
        batch = data[start:start + batch_size]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬ï¼Œå¯¹inference_contextè¿›è¡Œæˆªæ–­å¤„ç†
        input_texts = []
        for item in batch:
            # ä½¿ç”¨inference_contextè¿›è¡Œæ¨ç†
            inference_text = item.get('inference_context', item.get('context', ''))
            truncated_context = truncate_context(inference_text, tokenizer)
            
            # æ„å»ºprompt
            prompt_text = f"You are a helpful assistant. Please identify tags of user intentions in the following user query and provide an explanation for each tag. Please respond in the JSON format {{\"tag\": str, \"explanation\": str}}. Query: {truncated_context} Assistant:"
            input_texts.append(prompt_text)

        eos_tokens = []
        if tokenizer.eos_token:
            eos_tokens.append(tokenizer.eos_token)

        # æ·»åŠ å¸¸è§çš„åœæ­¢æ ‡è®°ä½œä¸ºå¤‡ç”¨
        stop_tokens = eos_tokens + ["</s>", "<end_of_turn>"]

        sampling_params = SamplingParams(
            temperature=0.9,
            top_p=0.6,
            max_tokens=512,
            stop=stop_tokens,
        )

        try:
            responses = llm.generate(input_texts, sampling_params)
        except Exception as e:
            print(f"Error during vLLM batch generation: {e}")
            responses = [None] * len(batch)

        # å¤„ç†æ‰¹æ¬¡çš„ç»“æœ
        current_batch_results = []
        for i, item in enumerate(batch):
            idx = start + i
            try:
                if responses[i] is not None:
                    raw_text = responses[i].outputs[0].text
            
                    prefix = input_texts[i]
                    if raw_text.startswith(prefix):
                        generated_part = raw_text[len(prefix):].strip()
                    else:
                        generated_part = raw_text

                    # æå–æ ‡ç­¾å’Œè§£é‡Š
                    parsed_tags_with_explanations = extract_tags_with_explanations(generated_part)
                else:
                    raw_text = "Generation failed"
                    parsed_tags_with_explanations = []

                #ä¿æŒåŸå§‹æ•°æ® + æ·»åŠ ä¸‰ä¸ªæ–°å­—æ®µ
                result = {
                    **item, 
                    'truncated_input': truncate_context(
                        item.get('inference_context', item.get('context', '')), tokenizer
                    ),
                    'generated_tags': parsed_tags_with_explanations,
                    'raw_response': raw_text if save_raw_text else None,
                }

                current_batch_results.append(result)

            except Exception as e:
                print(f"Error processing sample {idx+1}: {e}")
                result = {
                    **item,  # ä¿æŒåŸå§‹æ•°æ®
                    'truncated_input': '',
                    'generated_tags': [],
                    'raw_response': f"Error: {str(e)}" if save_raw_text else None,
                }
                current_batch_results.append(result)

        batch_results.extend(current_batch_results)
        total_processed += len(current_batch_results)
        
        if len(batch_results) >= save_interval or (start + batch_size) >= len(data):
            total_saved = append_results_to_file(output_file, batch_results)
            print(f"Progress: {total_processed}/{len(data)} processed, {total_saved} total saved")
            batch_results = []

    end_time = time.time()
    print(f"Inference took {end_time - start_time:.2f} seconds")
    
    try:
        with open(output_file, 'rb') as f:
            final_results = pickle.load(f)
            print(f"Final verification: {len(final_results)} samples in output file")
        
        print("="*80)
        print("INFERENCE RESULTS PREVIEW")
        print("="*80)

        success_count = sum(1 for r in final_results if r.get('generated_tags', []))
        print(f"Successfully processed: {success_count}/{len(final_results)} samples")
        
        new_results_start = max(0, len(final_results) - total_processed)
        preview_results = final_results[new_results_start:new_results_start + 3]
        
        for i, result in enumerate(preview_results):
            print(f"\n--- Sample {new_results_start + i + 1} ---")
            generated_tags = result.get('generated_tags', [])
            print(f"Generated Tags: {[tag_info['tag'] for tag_info in generated_tags]}")
            if generated_tags:
                print(f"First 2 Tags with Explanations: {generated_tags[:2]}")
            print("-" * 60)

        print("="*80)
        
        return final_results
        
    except Exception as e:
        print(f"Warning: Could not verify final results: {e}")
        return []

def load_single_json_file(json_file: str, num_samples: int = None):
    """ä»å•ä¸ª.jsonæˆ–.jsonlæ–‡ä»¶åŠ è½½æ•°æ®"""
    try:
        file_data = []
        
        with open(json_file, 'r', encoding='utf-8') as f:
            if json_file.endswith('.jsonl'):
                
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        item = json.loads(line)
                        processed_item = process_item(item, json_file, line_num)
                        if processed_item:
                            file_data.append(processed_item)
                    except json.JSONDecodeError as e:
                        print(f"Warning: JSON decode error at line {line_num} in {json_file}: {e}")
                        continue
            else:
                # JSONæ ¼å¼ï¼šæ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªJSONæ•°ç»„æˆ–å¯¹è±¡
                try:
                    data = json.load(f)
                    if isinstance(data, list):
                        for idx, item in enumerate(data, 1):
                            processed_item = process_item(item, json_file, idx)
                            if processed_item:
                                file_data.append(processed_item)
                    elif isinstance(data, dict):
                        processed_item = process_item(data, json_file, 1)
                        if processed_item:
                            file_data.append(processed_item)
                except json.JSONDecodeError as e:
                    print(f"Error: Invalid JSON in {json_file}: {e}")
                    return []
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if num_samples is not None and num_samples > 0:
            file_data = file_data[:num_samples]
        
        return file_data
    except Exception as e:
        print(f"Error loading {json_file}: {e}")
        return []

def process_item(item, file_name, line_num):
    """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
    if not isinstance(item, dict):
        print(f"Warning: Item at line {line_num} in {file_name} is not a dict")
        return None
    
    # ç¡®ä¿æœ‰inputå­—æ®µ
    if 'input' not in item:
        print(f"Warning: Line {line_num} in {file_name} missing 'input' field")
        return None
    
    # è·å–contextå’Œinputå­—æ®µï¼Œåˆ›å»ºç”¨äºæ¨ç†çš„context
    context_content = item.get('context', '')
    input_content = item['input']
    
    # è¿æ¥contextå’Œinputå­—æ®µï¼Œæ·»åŠ ä¸ºæ–°å­—æ®µç”¨äºæ¨ç†
    if context_content:
        item['inference_context'] = f"{input_content}\n\n{context_content}"
    else:
        item['inference_context'] = input_content
    
    return item

def find_json_files(data_dir: str):
    """æŸ¥æ‰¾æ‰€æœ‰.jsonå’Œ.jsonlæ–‡ä»¶"""
    if not os.path.exists(data_dir):
        print(f"Error: {data_dir} directory not found!")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰.jsonå’Œ.jsonlæ–‡ä»¶ - é€’å½’æœç´¢
    all_files = glob.glob(os.path.join(data_dir, "**", "*.json*"), recursive=True)
    
    if not all_files:
        print(f"No .json or .jsonl files found in {data_dir}")
        return []
    
    print(f"Found {len(all_files)} files:")
    for file in all_files:
        rel_path = os.path.relpath(file, data_dir)
        print(f"  - {rel_path}")
    
    return all_files

def process_single_file(model_path: str, json_file: str, output_dir: str, args):
    """å¤„ç†å•ä¸ªJSONæ–‡ä»¶"""
    # è®¡ç®—ç›¸å¯¹äºdata_dirçš„è·¯å¾„
    rel_path = os.path.relpath(json_file, args.data_dir)
    
    # æ„å»ºè¾“å‡ºè·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
    rel_dir = os.path.dirname(rel_path)
    file_basename = os.path.splitext(os.path.basename(rel_path))[0]
    
    # åˆ›å»ºå¯¹åº”çš„è¾“å‡ºå­ç›®å½•
    output_subdir = os.path.join(output_dir, rel_dir) if rel_dir else output_dir
    os.makedirs(output_subdir, exist_ok=True)
    
    output_file = os.path.join(output_subdir, f"{file_basename}_results.pkl")
    
    print(f"\n{'='*80}")
    print(f"Processing file: {json_file}")
    print(f"Output file: {output_file}")
    print(f"{'='*80}")
    
    # åŠ è½½å•ä¸ªæ–‡ä»¶çš„æ•°æ®
    data = load_single_json_file(json_file, args.num_samples)
    if not data:
        print(f"No data loaded from {json_file}!")
        return None
    
    print(f"Loaded {len(data)} samples from {os.path.basename(json_file)}")
    
    # è¿è¡Œæ¨ç†
    try:
        results = run_inference_vllm(
            model_path, 
            data, 
            output_file,
            args.save_raw_text, 
            save_interval=args.save_interval, 
            batch_size=args.batch_size,
            tensor_parallel_size=args.tensor_parallel_size
        )
        print(f"\nâœ“ Successfully processed {json_file}")
        print(f"  Results: {len(results)} samples")
        print(f"  Output saved to: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"âœ— Error processing {json_file}: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    parser = argparse.ArgumentParser(description="Inference with local model or HF model - generate tags with explanations for JSON/JSONL data")
    parser.add_argument("--checkpoint_path", type=str, required=True,
                        help="Local model directory path or Hugging Face model ID")
    parser.add_argument("--data_dir", type=str, default="data", 
                        help="Directory containing .json/.jsonl files (default: 'data')")
    parser.add_argument("--output_prefix", type=str, default="results",
                        help="Prefix for output directory name (default: 'results')")
    parser.add_argument("--num_samples", type=int, default=None,
                        help="Limit number of samples per file (default: process all)")
    parser.add_argument("--save_raw_text", action="store_true", default=True)
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size for inference")
    parser.add_argument("--save_interval", type=int, default=100, help="Save interval for incremental saves")
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="Number of GPUs for tensor parallel")

    args = parser.parse_args()
    
    # åˆ¤æ–­æ˜¯æœ¬åœ°æ¨¡å‹è¿˜æ˜¯HFæ¨¡å‹
    model_path = args.checkpoint_path
    is_hf_model = is_huggingface_model_id(model_path)

    if is_hf_model:
        print(f"Using Hugging Face model: {model_path}")
    else:
        print(f"Using local model: {model_path}")
        # åªå¯¹æœ¬åœ°æ¨¡å‹éªŒè¯è·¯å¾„å­˜åœ¨æ€§
        if not os.path.exists(model_path):
            print(f"Error: Local model path does not exist: {model_path}")
            return
    
    # åˆ›å»ºä»¥æ¨¡å‹åå‘½åçš„è¾“å‡ºç›®å½•
    model_name = get_model_name_from_path(model_path)
    output_dir = f"{args.output_prefix}_{model_name}"
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"Model path: {model_path}")
    print(f"Output directory: {output_dir}")
    print(f"Using data directory: {args.data_dir}")
    print(f"Batch size: {args.batch_size}")
    print(f"Tensor parallel size: {args.tensor_parallel_size}")
    print(f"Save interval: {args.save_interval}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(args.data_dir):
        print(f"Error: Data directory does not exist: {args.data_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    json_files = find_json_files(args.data_dir)
    if not json_files:
        print(f"No JSON files found in {args.data_dir}!")
        return
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    processed_files = []
    failed_files = []
    
    print(f"\nStarting to process {len(json_files)} files...")
    
    for i, json_file in enumerate(json_files, 1):
        print(f"\nProcessing file {i}/{len(json_files)}: {os.path.relpath(json_file, args.data_dir)}")
        
        result = process_single_file(model_path, json_file, output_dir, args)
        
        if result:
            processed_files.append(result)
        else:
            failed_files.append(json_file)
    
    # è¾“å‡ºæ€»ç»“
    print(f"\n{'='*80}")
    print("PROCESSING SUMMARY")
    print(f"{'='*80}")
    print(f"Total files: {len(json_files)}")
    print(f"Successfully processed: {len(processed_files)}")
    print(f"Failed: {len(failed_files)}")
    
    if processed_files:
        print(f"\nSuccessfully processed files:")
        for output_file in processed_files:
            print(f"  - {output_file}")
    
    if failed_files:
        print(f"\nFailed files:")
        for failed_file in failed_files:
            print(f"  - {failed_file}")
    
    print(f"\nğŸ“‚ All results saved in directory: {output_dir}")
    print("âœ¨ Processing completed!")

if __name__ == "__main__":
    main()